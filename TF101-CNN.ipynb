{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Convolutional Neural Network, CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN Model Class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32,             # 卷積層神經元（卷積核）數目\n",
    "            kernel_size=[5, 5],     # 接受區的大小\n",
    "            padding='same',         # padding策略（vaild 或 same）\n",
    "            activation=tf.nn.relu   # 激活函数\n",
    "        )\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\n",
    "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
    "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
    "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
    "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
    "        x = self.dense1(x)                      # [batch_size, 1024]\n",
    "        x = self.dense2(x)                      # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的圖片預設為uint8（0-255的數字）。以下程式碼將其正規化到0-1之間的浮點數，並在最後增加一維作為顏色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 從資料集中隨機取出batch_size個元素並返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.319341\n",
      "batch 100: loss 0.028434\n",
      "batch 200: loss 0.108907\n",
      "batch 300: loss 0.204677\n",
      "batch 400: loss 0.020931\n",
      "batch 500: loss 0.027794\n",
      "batch 600: loss 0.006845\n",
      "batch 700: loss 0.015851\n",
      "batch 800: loss 0.010765\n",
      "batch 900: loss 0.032053\n",
      "batch 1000: loss 0.087948\n",
      "batch 1100: loss 0.045569\n",
      "batch 1200: loss 0.001384\n",
      "batch 1300: loss 0.048368\n",
      "batch 1400: loss 0.006931\n",
      "batch 1500: loss 0.001659\n",
      "batch 1600: loss 0.001608\n",
      "batch 1700: loss 0.042767\n",
      "batch 1800: loss 0.032545\n",
      "batch 1900: loss 0.003365\n",
      "batch 2000: loss 0.030996\n",
      "batch 2100: loss 0.045691\n",
      "batch 2200: loss 0.002667\n",
      "batch 2300: loss 0.034235\n",
      "batch 2400: loss 0.001207\n",
      "batch 2500: loss 0.003352\n",
      "batch 2600: loss 0.000366\n",
      "batch 2700: loss 0.059833\n",
      "batch 2800: loss 0.007274\n",
      "batch 2900: loss 0.042182\n",
      "batch 3000: loss 0.050765\n",
      "batch 3100: loss 0.004097\n",
      "batch 3200: loss 0.002165\n",
      "batch 3300: loss 0.005605\n",
      "batch 3400: loss 0.003715\n",
      "batch 3500: loss 0.013883\n",
      "batch 3600: loss 0.019366\n",
      "batch 3700: loss 0.037358\n",
      "batch 3800: loss 0.017495\n",
      "batch 3900: loss 0.000794\n",
      "batch 4000: loss 0.017433\n",
      "batch 4100: loss 0.001454\n",
      "batch 4200: loss 0.011447\n",
      "batch 4300: loss 0.001480\n",
      "batch 4400: loss 0.002546\n",
      "batch 4500: loss 0.002886\n",
      "batch 4600: loss 0.012208\n",
      "batch 4700: loss 0.012328\n",
      "batch 4800: loss 0.000235\n",
      "batch 4900: loss 0.002648\n",
      "batch 5000: loss 0.000238\n",
      "batch 5100: loss 0.000010\n",
      "batch 5200: loss 0.012431\n",
      "batch 5300: loss 0.001175\n",
      "batch 5400: loss 0.001808\n",
      "batch 5500: loss 0.000335\n",
      "batch 5600: loss 0.010803\n",
      "batch 5700: loss 0.035264\n",
      "batch 5800: loss 0.000158\n",
      "batch 5900: loss 0.011157\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "model = CNN()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        if batch_index %100 == 0:\n",
    "            print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.991400\n"
     ]
    }
   ],
   "source": [
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use build-in model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
      "14540800/14536120 [==============================] - 3s 0us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\u001B[1mDownloading and preparing dataset tf_flowers/3.0.1 (download: 218.21 MiB, generated: 221.83 MiB, total: 440.05 MiB) to /Users/clarence/tensorflow_datasets/tf_flowers/3.0.1...\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1mDataset tf_flowers downloaded and prepared to /Users/clarence/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\u001B[0m\n",
      "loss 1.667159\n",
      "loss 1.529997\n",
      "loss 2.027193\n",
      "loss 1.586838\n",
      "loss 1.684473\n",
      "loss 1.615139\n",
      "loss 1.649708\n",
      "loss 1.682875\n",
      "loss 1.672987\n",
      "loss 1.421459\n",
      "loss 1.408737\n",
      "loss 1.472062\n",
      "loss 1.259190\n",
      "loss 1.387612\n",
      "loss 1.286047\n",
      "loss 1.137236\n",
      "loss 1.644343\n",
      "loss 1.018014\n",
      "loss 1.285634\n",
      "loss 1.284042\n",
      "loss 1.428361\n",
      "loss 1.638035\n",
      "loss 1.671472\n",
      "loss 1.642432\n",
      "loss 1.466140\n",
      "loss 1.547979\n",
      "loss 1.377649\n",
      "loss 1.090657\n",
      "loss 1.224371\n",
      "loss 1.259674\n",
      "loss 1.308285\n",
      "loss 1.305055\n",
      "loss 1.560040\n",
      "loss 1.397419\n",
      "loss 1.190457\n",
      "loss 1.326029\n",
      "loss 1.290420\n",
      "loss 1.341389\n",
      "loss 1.345169\n",
      "loss 1.523286\n",
      "loss 1.244739\n",
      "loss 1.537754\n",
      "loss 1.180762\n",
      "loss 1.376679\n",
      "loss 1.468620\n",
      "loss 1.531018\n",
      "loss 1.498878\n",
      "loss 0.950563\n",
      "loss 1.257014\n",
      "loss 1.435990\n",
      "loss 1.294153\n",
      "loss 1.139748\n",
      "loss 1.610894\n",
      "loss 1.066491\n",
      "loss 1.137355\n",
      "loss 1.222409\n",
      "loss 1.099669\n",
      "loss 1.447250\n",
      "loss 1.314126\n",
      "loss 1.283808\n",
      "loss 1.091633\n",
      "loss 1.314233\n",
      "loss 1.306846\n",
      "loss 1.442396\n",
      "loss 1.495053\n",
      "loss 1.116490\n",
      "loss 1.151206\n",
      "loss 1.058774\n",
      "loss 1.227613\n",
      "loss 1.162186\n",
      "loss 1.173297\n",
      "loss 0.925137\n",
      "loss 1.255702\n",
      "loss 1.056526\n",
      "tf.Tensor(\n",
      "[[0.01522316 0.00913506 0.41285342 0.01715526 0.5456331 ]\n",
      " [0.7463138  0.10478325 0.053906   0.02779319 0.0672038 ]\n",
      " [0.00430906 0.0149104  0.6860552  0.03086039 0.26386496]\n",
      " [0.00483154 0.01085725 0.63382214 0.0174771  0.33301196]\n",
      " [0.23809451 0.09706376 0.2123801  0.35198894 0.10047275]\n",
      " [0.00996658 0.00714328 0.6429915  0.01391114 0.3259875 ]\n",
      " [0.22033761 0.13318329 0.05404716 0.5782027  0.01422918]\n",
      " [0.87040085 0.03791077 0.02476355 0.04210518 0.02481975]\n",
      " [0.14447111 0.588358   0.04830049 0.0226904  0.19617999]\n",
      " [0.18852945 0.31213647 0.14581925 0.18080601 0.17270878]\n",
      " [0.00313437 0.00741261 0.7812148  0.01014677 0.1980915 ]\n",
      " [0.35116896 0.07896914 0.23627506 0.06719249 0.26639432]\n",
      " [0.00094646 0.00120369 0.71338105 0.00762489 0.27684385]\n",
      " [0.8933205  0.03558856 0.02594978 0.01707264 0.02806856]\n",
      " [0.10161673 0.00967237 0.02843599 0.8532107  0.0070642 ]\n",
      " [0.42879513 0.02932666 0.04980535 0.47577322 0.01629954]\n",
      " [0.21572255 0.0216686  0.0546911  0.6952096  0.01270813]\n",
      " [0.16169861 0.36010158 0.19043203 0.10774171 0.18002611]\n",
      " [0.40086    0.09311605 0.2634891  0.0563231  0.18621165]\n",
      " [0.931526   0.02387548 0.01618618 0.01145456 0.01695785]], shape=(20, 5), dtype=float32)\n",
      "loss 1.105467\n",
      "loss 1.012406\n",
      "loss 1.361551\n",
      "loss 1.226324\n",
      "loss 1.024823\n",
      "loss 1.149561\n",
      "loss 1.147589\n",
      "loss 1.134443\n",
      "loss 1.099897\n",
      "loss 1.471111\n",
      "loss 0.982254\n",
      "loss 1.143782\n",
      "loss 1.169783\n",
      "loss 1.312997\n",
      "loss 1.078370\n",
      "loss 1.136898\n",
      "loss 1.394560\n",
      "loss 1.141492\n",
      "loss 1.009296\n",
      "loss 1.068340\n",
      "loss 0.984722\n",
      "loss 1.152528\n",
      "loss 0.920232\n",
      "loss 0.952171\n",
      "loss 1.021587\n",
      "loss 0.859845\n",
      "loss 1.051408\n",
      "loss 1.423984\n",
      "loss 1.125315\n",
      "loss 1.037273\n",
      "loss 0.993583\n",
      "loss 1.143945\n",
      "loss 0.969647\n",
      "loss 0.847486\n",
      "loss 1.048159\n",
      "loss 1.265692\n",
      "loss 0.985437\n",
      "loss 0.899963\n",
      "loss 1.395139\n",
      "loss 0.861195\n",
      "loss 0.886414\n",
      "loss 1.103023\n",
      "loss 0.910849\n",
      "loss 0.869367\n",
      "loss 0.917497\n",
      "loss 1.147992\n",
      "loss 1.149585\n",
      "loss 1.228865\n",
      "loss 1.133164\n",
      "loss 1.026397\n",
      "loss 0.898528\n",
      "loss 0.866565\n",
      "loss 1.064680\n",
      "loss 1.022759\n",
      "loss 1.004586\n",
      "loss 1.135061\n",
      "loss 0.584892\n",
      "loss 0.888888\n",
      "loss 1.057536\n",
      "loss 0.683929\n",
      "loss 0.848331\n",
      "loss 1.044816\n",
      "loss 1.039920\n",
      "loss 0.878630\n",
      "loss 1.143262\n",
      "loss 0.903749\n",
      "loss 0.705818\n",
      "loss 0.916589\n",
      "loss 0.963866\n",
      "loss 0.898602\n",
      "loss 0.705618\n",
      "loss 0.941899\n",
      "loss 0.845161\n",
      "loss 0.768062\n",
      "tf.Tensor(\n",
      "[[3.37252945e-01 1.27313687e-02 2.76163276e-02 6.18439257e-01\n",
      "  3.96005344e-03]\n",
      " [9.15827882e-03 3.40839699e-02 1.46199122e-01 2.09617186e-02\n",
      "  7.89596915e-01]\n",
      " [4.40542102e-02 9.24426243e-02 2.13023767e-01 7.80130178e-02\n",
      "  5.72466433e-01]\n",
      " [9.80017304e-01 1.42289149e-02 2.54167686e-03 1.28591910e-03\n",
      "  1.92613248e-03]\n",
      " [9.57224611e-03 9.10221517e-01 1.70302670e-02 2.70808786e-02\n",
      "  3.60950530e-02]\n",
      " [4.36143838e-02 3.37696560e-02 1.66010395e-01 3.67519371e-02\n",
      "  7.19853580e-01]\n",
      " [4.50281322e-01 4.31656510e-01 3.52921598e-02 3.01630180e-02\n",
      "  5.26069067e-02]\n",
      " [1.48322431e-06 3.11437507e-05 7.00201511e-01 3.63522413e-04\n",
      "  2.99402356e-01]\n",
      " [6.56222641e-01 2.55030215e-01 3.22714522e-02 1.41503606e-02\n",
      "  4.23252173e-02]\n",
      " [8.63834247e-02 3.25566879e-03 3.39328498e-02 8.74575496e-01\n",
      "  1.85256859e-03]\n",
      " [1.45687073e-01 9.31511633e-03 6.78304285e-02 7.71411955e-01\n",
      "  5.75547013e-03]\n",
      " [1.21521745e-02 4.89511935e-04 2.27888599e-02 9.64159191e-01\n",
      "  4.10268869e-04]\n",
      " [3.44822079e-01 2.36047685e-01 1.10870741e-01 2.47037888e-01\n",
      "  6.12216257e-02]\n",
      " [2.98207771e-04 4.42143762e-03 5.35052955e-01 7.78822275e-03\n",
      "  4.52439249e-01]\n",
      " [8.06336820e-01 1.32991314e-01 1.39796045e-02 7.00851530e-03\n",
      "  3.96837704e-02]\n",
      " [1.12192203e-04 4.20936756e-03 5.04961312e-01 6.45748433e-03\n",
      "  4.84259635e-01]\n",
      " [3.13013829e-02 6.73617244e-01 5.82197346e-02 1.96941569e-01\n",
      "  3.99199761e-02]\n",
      " [5.77785680e-03 2.05970049e-04 1.67588349e-02 9.77079868e-01\n",
      "  1.77356051e-04]\n",
      " [9.22207832e-01 4.37512398e-02 9.05308221e-03 9.12466459e-03\n",
      "  1.58631355e-02]\n",
      " [2.27283966e-03 1.39121423e-02 2.30218515e-01 5.98638458e-03\n",
      "  7.47610152e-01]], shape=(20, 5), dtype=float32)\n",
      "loss 0.834228\n",
      "loss 1.210023\n",
      "loss 0.984113\n",
      "loss 0.980568\n",
      "loss 0.839469\n",
      "loss 0.978463\n",
      "loss 0.861965\n",
      "loss 0.862038\n",
      "loss 0.751417\n",
      "loss 0.798832\n",
      "loss 0.951264\n",
      "loss 0.781434\n",
      "loss 1.138662\n",
      "loss 0.876391\n",
      "loss 0.757223\n",
      "loss 0.950115\n",
      "loss 1.282677\n",
      "loss 0.880792\n",
      "loss 0.899035\n",
      "loss 0.815707\n",
      "loss 0.820063\n",
      "loss 0.902636\n",
      "loss 0.996214\n",
      "loss 0.956915\n",
      "loss 0.767325\n",
      "loss 0.829559\n",
      "loss 0.827487\n",
      "loss 1.183955\n",
      "loss 0.931874\n",
      "loss 0.661209\n",
      "loss 0.975944\n",
      "loss 0.983613\n",
      "loss 0.861235\n",
      "loss 0.824837\n",
      "loss 0.797680\n",
      "loss 0.948719\n",
      "loss 1.053641\n",
      "loss 0.928591\n",
      "loss 0.865206\n",
      "loss 1.068210\n",
      "loss 1.005857\n",
      "loss 0.834893\n",
      "loss 0.941111\n",
      "loss 1.202069\n",
      "loss 0.685403\n",
      "loss 0.889748\n",
      "loss 0.719559\n",
      "loss 0.911780\n",
      "loss 0.737131\n",
      "loss 0.819102\n",
      "loss 0.507323\n",
      "loss 0.911725\n",
      "loss 1.069174\n",
      "loss 0.725168\n",
      "loss 0.968250\n",
      "loss 0.891264\n",
      "loss 0.825924\n",
      "loss 0.857323\n",
      "loss 1.029063\n",
      "loss 0.760916\n",
      "loss 0.934293\n",
      "loss 1.230404\n",
      "loss 0.646691\n",
      "loss 0.687048\n",
      "loss 0.890820\n",
      "loss 0.899164\n",
      "loss 0.613074\n",
      "loss 0.980700\n",
      "loss 0.991541\n",
      "loss 0.826290\n",
      "loss 0.635884\n",
      "loss 0.811428\n",
      "loss 0.957645\n",
      "loss 1.443126\n",
      "tf.Tensor(\n",
      "[[2.20885739e-01 5.75705886e-01 3.17260250e-02 8.75210948e-03\n",
      "  1.62930250e-01]\n",
      " [2.52983766e-03 1.87508352e-02 6.74493432e-01 1.65717918e-02\n",
      "  2.87654042e-01]\n",
      " [1.65575147e-02 9.50484574e-01 7.30400300e-03 1.38704199e-03\n",
      "  2.42670290e-02]\n",
      " [1.18711451e-02 5.32569326e-02 5.27894855e-01 3.01529709e-02\n",
      "  3.76824111e-01]\n",
      " [1.22318789e-01 2.23511845e-01 1.45576343e-01 3.89676131e-02\n",
      "  4.69625473e-01]\n",
      " [1.81631092e-02 8.35098743e-01 2.90915724e-02 5.34544187e-03\n",
      "  1.12301163e-01]\n",
      " [4.71154451e-01 4.40389305e-01 1.50490142e-02 1.53274450e-03\n",
      "  7.18744621e-02]\n",
      " [3.39849181e-02 5.38631517e-04 1.19306194e-02 9.52887058e-01\n",
      "  6.58717763e-04]\n",
      " [4.73436676e-02 8.90633106e-01 7.70871621e-03 1.91299245e-03\n",
      "  5.24014756e-02]\n",
      " [1.04010792e-03 4.77611041e-03 4.64805901e-01 1.01287365e-02\n",
      "  5.19249201e-01]\n",
      " [1.18787838e-02 2.38469932e-02 5.92929006e-01 7.85804074e-03\n",
      "  3.63487124e-01]\n",
      " [5.51552415e-01 2.37193570e-01 4.93702926e-02 2.45108083e-02\n",
      "  1.37372881e-01]\n",
      " [1.15001947e-01 7.90665150e-01 1.46419713e-02 1.24026486e-03\n",
      "  7.84505829e-02]\n",
      " [1.32640656e-02 3.37853213e-04 3.68405855e-03 9.82444227e-01\n",
      "  2.69881799e-04]\n",
      " [1.41957542e-02 6.58613397e-04 1.01030935e-02 9.74204838e-01\n",
      "  8.37628962e-04]\n",
      " [1.68818678e-03 3.69380414e-03 9.16560650e-01 3.39986570e-03\n",
      "  7.46575445e-02]\n",
      " [1.91505924e-02 1.11478451e-03 2.53120996e-03 9.76546288e-01\n",
      "  6.57098368e-04]\n",
      " [3.15592080e-01 5.03786206e-01 3.29709426e-02 7.99814425e-03\n",
      "  1.39652580e-01]\n",
      " [3.31791252e-01 3.63776125e-02 1.25387952e-01 4.85574037e-01\n",
      "  2.08690874e-02]\n",
      " [7.90087044e-01 1.49294183e-01 9.95137822e-03 1.23761769e-03\n",
      "  4.94298190e-02]], shape=(20, 5), dtype=float32)\n",
      "loss 0.901335\n",
      "loss 0.897020\n",
      "loss 0.887853\n",
      "loss 0.722245\n",
      "loss 0.574096\n",
      "loss 0.614348\n",
      "loss 0.930269\n",
      "loss 0.929529\n",
      "loss 0.643361\n",
      "loss 1.139870\n",
      "loss 0.755159\n",
      "loss 0.618776\n",
      "loss 0.851711\n",
      "loss 0.666008\n",
      "loss 0.620377\n",
      "loss 0.799671\n",
      "loss 0.743911\n",
      "loss 0.774778\n",
      "loss 0.857845\n",
      "loss 0.982580\n",
      "loss 1.035868\n",
      "loss 0.556556\n",
      "loss 0.680384\n",
      "loss 0.480802\n",
      "loss 0.937800\n",
      "loss 0.723377\n",
      "loss 0.792556\n",
      "loss 0.752528\n",
      "loss 0.927024\n",
      "loss 0.752223\n",
      "loss 0.859900\n",
      "loss 0.788605\n",
      "loss 0.522399\n",
      "loss 0.581284\n",
      "loss 0.820297\n",
      "loss 0.669774\n",
      "loss 0.777040\n",
      "loss 0.858192\n",
      "loss 0.713273\n",
      "loss 0.666978\n",
      "loss 0.745241\n",
      "loss 0.638765\n",
      "loss 0.858447\n",
      "loss 0.830441\n",
      "loss 0.884974\n",
      "loss 0.895850\n",
      "loss 0.828706\n",
      "loss 1.086850\n",
      "loss 0.864327\n",
      "loss 0.931002\n",
      "loss 0.558725\n",
      "loss 0.922737\n",
      "loss 0.926348\n",
      "loss 0.702571\n",
      "loss 0.964648\n",
      "loss 0.752638\n",
      "loss 1.209507\n",
      "loss 0.715804\n",
      "loss 0.751016\n",
      "loss 1.053349\n",
      "loss 0.934471\n",
      "loss 0.972779\n",
      "loss 0.865683\n",
      "loss 1.036302\n",
      "loss 0.688845\n",
      "loss 0.839012\n",
      "loss 0.627395\n",
      "loss 0.739913\n",
      "loss 0.670423\n",
      "loss 0.749680\n",
      "loss 0.952167\n",
      "loss 0.926152\n",
      "loss 0.772356\n",
      "loss 0.678946\n",
      "tf.Tensor(\n",
      "[[2.58186162e-02 2.70646065e-01 1.32932246e-01 4.86108273e-01\n",
      "  8.44948292e-02]\n",
      " [2.55194213e-03 1.28218113e-02 7.03941166e-01 2.80499421e-02\n",
      "  2.52635121e-01]\n",
      " [8.50643516e-01 6.80457652e-02 2.07700115e-02 2.57303845e-02\n",
      "  3.48103456e-02]\n",
      " [3.37517150e-02 1.82513036e-02 5.42230085e-02 8.81606281e-01\n",
      "  1.21676913e-02]\n",
      " [1.25510982e-04 2.82120565e-03 8.86631072e-01 3.71184573e-03\n",
      "  1.06710255e-01]\n",
      " [5.94850659e-01 2.05015063e-01 5.24075478e-02 1.22038856e-01\n",
      "  2.56878976e-02]\n",
      " [4.60045924e-03 3.55579032e-05 1.04533611e-02 9.84838605e-01\n",
      "  7.20198223e-05]\n",
      " [4.76092845e-03 8.93274508e-03 6.10652268e-01 2.54215151e-01\n",
      "  1.21438935e-01]\n",
      " [9.30642426e-01 4.27732393e-02 5.31150261e-03 8.10510106e-03\n",
      "  1.31675638e-02]\n",
      " [2.64102891e-02 2.13462468e-02 3.32852900e-01 5.54334402e-01\n",
      "  6.50561973e-02]\n",
      " [2.56871611e-01 2.86236018e-01 1.48591623e-01 3.19670364e-02\n",
      "  2.76333690e-01]\n",
      " [3.34679425e-01 1.06013091e-02 4.52586263e-02 6.05182946e-01\n",
      "  4.27765632e-03]\n",
      " [7.85829067e-01 4.27912921e-02 1.25467330e-02 1.57162964e-01\n",
      "  1.66987791e-03]\n",
      " [2.86540985e-01 1.02604739e-02 6.33057952e-02 6.32608354e-01\n",
      "  7.28438562e-03]\n",
      " [8.80018353e-01 6.94620833e-02 9.17499140e-03 1.13061154e-02\n",
      "  3.00384723e-02]\n",
      " [2.41158418e-02 9.73156273e-01 8.20636342e-04 5.85966045e-04\n",
      "  1.32121914e-03]\n",
      " [1.10996775e-02 5.40617049e-01 1.64395586e-01 2.64067277e-02\n",
      "  2.57480919e-01]\n",
      " [2.54486501e-03 5.07445112e-02 3.38740051e-02 5.98849496e-03\n",
      "  9.06848133e-01]\n",
      " [1.69132762e-02 5.91472201e-02 5.98884761e-01 1.76012274e-02\n",
      "  3.07453573e-01]\n",
      " [4.34708549e-04 8.29437934e-03 5.96604466e-01 3.88763263e-03\n",
      "  3.90778810e-01]], shape=(20, 5), dtype=float32)\n",
      "loss 0.433147\n",
      "loss 0.610728\n",
      "loss 0.672780\n",
      "loss 1.025956\n",
      "loss 0.947947\n",
      "loss 0.726030\n",
      "loss 0.765801\n",
      "loss 0.773708\n",
      "loss 0.689628\n",
      "loss 0.506639\n",
      "loss 0.682066\n",
      "loss 0.977693\n",
      "loss 0.849907\n",
      "loss 0.698786\n",
      "loss 0.552248\n",
      "loss 0.604090\n",
      "loss 0.541249\n",
      "loss 0.601209\n",
      "loss 0.677874\n",
      "loss 0.734701\n",
      "loss 0.493966\n",
      "loss 0.670178\n",
      "loss 0.663485\n",
      "loss 0.553954\n",
      "loss 0.762142\n",
      "loss 0.705006\n",
      "loss 0.607612\n",
      "loss 0.673153\n",
      "loss 0.613000\n",
      "loss 0.571317\n",
      "loss 0.663163\n",
      "loss 0.799079\n",
      "loss 0.656014\n",
      "loss 0.587487\n",
      "loss 0.441295\n",
      "loss 0.624107\n",
      "loss 0.588869\n",
      "loss 0.691528\n",
      "loss 0.802384\n",
      "loss 0.671154\n",
      "loss 0.603736\n",
      "loss 0.580138\n",
      "loss 0.776968\n",
      "loss 0.522551\n",
      "loss 0.598809\n",
      "loss 0.764739\n",
      "loss 0.941346\n",
      "loss 0.848459\n",
      "loss 0.707832\n",
      "loss 0.980785\n",
      "loss 0.642174\n",
      "loss 0.720644\n",
      "loss 0.771764\n",
      "loss 0.679950\n",
      "loss 0.652565\n",
      "loss 0.884360\n",
      "loss 0.792176\n",
      "loss 0.594895\n",
      "loss 0.582475\n",
      "loss 0.943009\n",
      "loss 0.702326\n",
      "loss 0.860549\n",
      "loss 0.520678\n",
      "loss 0.445372\n",
      "loss 0.640050\n",
      "loss 0.491852\n",
      "loss 0.664023\n",
      "loss 0.666361\n",
      "loss 0.791914\n",
      "loss 0.976867\n",
      "loss 0.638012\n",
      "loss 0.710865\n",
      "loss 0.711937\n",
      "loss 0.561725\n",
      "tf.Tensor(\n",
      "[[1.30810213e-05 9.99511242e-01 3.89400841e-04 5.30061334e-05\n",
      "  3.32958989e-05]\n",
      " [1.59042910e-01 3.98498848e-02 7.96227902e-02 7.12009847e-01\n",
      "  9.47462860e-03]\n",
      " [8.04462790e-01 6.03727847e-02 2.16753893e-02 6.10916056e-02\n",
      "  5.23974076e-02]\n",
      " [1.35975674e-01 1.02882005e-01 2.55448431e-01 9.77162942e-02\n",
      "  4.07977581e-01]\n",
      " [6.93301612e-04 1.28765441e-05 3.62366368e-03 9.95664537e-01\n",
      "  5.56978557e-06]\n",
      " [2.52380013e-01 4.55397576e-01 8.83515403e-02 9.15086083e-03\n",
      "  1.94720060e-01]\n",
      " [3.97550352e-02 4.53206711e-03 7.12069198e-02 8.81649554e-01\n",
      "  2.85641896e-03]\n",
      " [7.41944537e-02 2.62062089e-03 1.21423202e-02 9.10397947e-01\n",
      "  6.44652406e-04]\n",
      " [2.18618289e-03 4.50879429e-03 4.68921930e-01 4.31053340e-03\n",
      "  5.20072579e-01]\n",
      " [3.47571298e-02 3.09503861e-02 3.83732468e-02 5.07853329e-02\n",
      "  8.45133901e-01]\n",
      " [1.45882726e-01 2.05550686e-01 2.69546419e-01 1.20536372e-01\n",
      "  2.58483827e-01]\n",
      " [1.64599151e-05 1.16891992e-04 1.11455098e-01 1.13229116e-03\n",
      "  8.87279272e-01]\n",
      " [6.47461957e-06 6.15571480e-05 9.03572619e-01 9.38935729e-04\n",
      "  9.54204053e-02]\n",
      " [9.94160056e-01 2.44017481e-03 3.11933341e-04 4.72181360e-04\n",
      "  2.61573540e-03]\n",
      " [1.04617083e-03 3.45353433e-03 9.09748971e-01 2.76159728e-03\n",
      "  8.29896554e-02]\n",
      " [9.68259454e-01 1.61968023e-02 1.70009129e-03 2.81042582e-03\n",
      "  1.10332249e-02]\n",
      " [3.90885724e-03 3.53630967e-02 3.46591532e-01 5.17232064e-03\n",
      "  6.08964145e-01]\n",
      " [1.83453411e-01 6.46304846e-01 5.30823208e-02 2.46536098e-02\n",
      "  9.25057903e-02]\n",
      " [4.09987599e-01 2.16517046e-01 2.19418313e-02 7.16658235e-02\n",
      "  2.79887646e-01]\n",
      " [8.45103979e-01 2.49608885e-02 2.28246078e-02 6.09046258e-02\n",
      "  4.62057963e-02]], shape=(20, 5), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Dataset tf_flowers is hosted on GCS. It will automatically be downloaded to your\n",
      "local data directory. If you'd instead prefer to read directly from our public\n",
      "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
      "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=5.0, style=ProgressStyle(descriptio…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c70b597bd0df4887929df1e08fb6f7f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = tf.keras.applications.MobileNetV2()\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "num_epoch = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = tfds.load(\"tf_flowers\", split=tfds.Split.TRAIN, as_supervised=True)\n",
    "dataset = dataset.map(lambda img, label: (tf.image.resize(img, (224, 224)) / 255.0, label)).shuffle(1024).batch(batch_size)\n",
    "model = tf.keras.applications.MobileNetV2(weights=None, classes=5)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "for e in range(num_epoch):\n",
    "    for images, labels in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            labels_pred = model(images, training=True)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=labels, y_pred=labels_pred)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            print(\"loss %f\" % loss.numpy())\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))\n",
    "    print(labels_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}